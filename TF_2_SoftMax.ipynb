{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "softmax模型可以用来给不同的对象分配概率。<br>\n",
    "在训练更加复杂的模型时，最后一步也往往需要用softmax来分配概率。<br>\n",
    "\n",
    "softmax回归分两步：\n",
    "* 首先 对某个待分类对象属于某个类的“证据”相加求和\n",
    "* 然后 将这个“证据”的和和转化为概率\n",
    "\n",
    "例如计算一张图片是否属于某类。<br>\n",
    "使用加权的方法来累积计算这张图是否属于某类的“证据”。<br>\n",
    "如果图片的像素强有力的体现该图不属于某个类，则权重的数值为负数。<br>\n",
    "相反，则权重的数值为正。<br>\n",
    "还需要引入额外的“证据”，称之为偏置量(bias)<br>\n",
    "因此对于给定的输入图片$x$是属于第$i$类的总体“证据”可以表示为：\n",
    "\n",
    "$$\n",
    "evidence_i = \\sum_{i}W_{i,j}x_j + b_i\n",
    "$$\n",
    "\n",
    "其中$W_i$代表权重，$b_i$代表第$i$类的偏置量，$j$代表给定图片$x$的像素索引 用于求和<br>\n",
    "然后利用softmax函数可以把这些“证据”转化成概率$y$\n",
    "\n",
    "$$\n",
    "y = softmax(evidence)\n",
    "$$\n",
    "\n",
    "给定一张图$x$，它对于每一个类别的吻合度可以被softmax函数转换成一个概率值<br>\n",
    "softmax函数可以定义为：\n",
    "\n",
    "$$\n",
    "softmax(x) = normalize(exp(x))\n",
    "$$\n",
    "\n",
    "展开右边可以得到：\n",
    "\n",
    "$$\n",
    "softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j}exp(x_j)}\n",
    "$$\n",
    "\n",
    "假设模型里的权值不可以是0或是负数，softmax会正则化这些权重值，<br>\n",
    "是它们的总和等于1，以此构建一个有效率的概率分布。\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3\n",
    "\\end{matrix}\n",
    "\\right] = softmax\\left(\n",
    "\\begin{matrix}\n",
    "W_{1,1}x_1 + W_{1,2}x_1 + W_{1,3}x_1 + b_1 \\\\\n",
    "W_{2,1}x_2 + W_{2,2}x_2 + W_{2,3}x_2 + b_2 \\\\\n",
    "W_{3,1}x_3 + W_{3,2}x_3 + W_{3,3}x_3 + b_3\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "转换成用 矩阵乘法和向量相加来表示：\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3\n",
    "\\end{matrix}\n",
    "\\right] = softmax\\left(\n",
    "\\left[\\begin{matrix}\n",
    "W_{1,1}&W_{1,2}&W_{1,3} \\\\\n",
    "W_{2,1}&W_{2,2}&W_{2,3} \\\\\n",
    "W_{3,1}&W_{3,2}&W_{3,3}\n",
    "\\end{matrix}\\right]\n",
    "\\cdot\n",
    "\\left[\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{matrix}\\right]\n",
    "+\n",
    "\\left[\\begin{matrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3\n",
    "\\end{matrix}\\right]\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "简化为：\n",
    "$$\n",
    "y = softmax(W_x+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型\n",
    "为了训练模型，通常需要定义一个指标来评估模型的好坏。<br>\n",
    "这个指标被称为成本(cost)或是损失(loss)，两者意思相同，然后尽量最小化这个指标。<br>\n",
    "非常常见的成本函数是“交叉熵”(cross-entropy):\n",
    "\n",
    "$$\n",
    "H_{y'}(y) = -\\sum_{i}y'_i\\log(y_i)\n",
    "$$\n",
    "\n",
    "其中y是预测的概率分布。<br>\n",
    "y‘是实际的分布。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算交叉熵\n",
    "（重要）<br>\n",
    "交叉熵不仅仅用来衡量单一的一对预测和真实值，<br>\n",
    "而是所有图片的交叉熵的总和。<br>\n",
    "对100个数据点的预测的表示比单一数据点的预测的表示能更好的描述模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-89029161ec98>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "import numpy as np\n",
    "import time\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.参数设置\n",
    "\n",
    "n_input => MNIST数据的图片特征个数为(784个)： 28 x 28 = 784<br>\n",
    "n_classes => MNIST数据的种类个数为(10个)：0到9<br>\n",
    "learning_rate => 学习速率：比较好的策略是先设置为0.25,然后在训练到20个Epoch时改为0.025。学习速率太大时会导致代价函数振荡；学习速率太小时会导致收敛过慢。<br>\n",
    "batch_size => 批尺寸(批大小)：太大，权重的更新就不会那么频繁，优化过程太漫长；太小，计算的加速效果越不明显。<br>\n",
    "total_batch => 1个epoch需要的迭代次数(550次)。5500 / 100 = 550<br>\n",
    "training_epochs => 利用训练数据学习多少遍(10遍)。1个epoch意味着训练数据被用过1遍。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_classes = 10\n",
    "learning_rate = 0.25\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "training_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.创建模型\n",
    "\n",
    "x => 是一个占位符，用来保存输入数据。类型时float32；第一个维度None表示可以是任何长度(输入图片的个数)；第二个维度784是每个图片展平后的向量。<br>\n",
    "W => 是一个占位符，用来保存权重值。Variable常用来保存参数，计算中可以被修改，初始值为0。<br>\n",
    "b => 是一个占位符，用来保存偏置量。<br>\n",
    "y => 是预测出来的分类结果，还未softmax！！！<br>\n",
    "y_pred => 是经过softmax之后的预测出来的分类结果。<br>\n",
    "y_softm => 是一个占位符，用来保存输出数据的正确值(正确分类)。<br>\n",
    "cross_entropy => 交叉熵，又可以叫做cost，有3种代码写作方式：第1种是直接按照公式写出的代码，如果出现log(0)的话结果就会变成Nan，不建议使用；第2种...with_logits用于较低版本的tf；第3种...with_logits_v2用于较新的tf。<br>\n",
    "train_step => 学习步伐，又可叫做optimizer，永华方法使用梯度下降算法来最小化交叉熵，会自动使用反向传播法。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "W = tf.Variable(tf.zeros([n_input, n_classes]))\n",
    "b = tf.Variable(tf.zeros([n_classes]))\n",
    "y_pred = tf.matmul(x, W) + b\n",
    "y_softmax = tf.nn.softmax(y_pred)\n",
    "y_true = tf.placeholder(tf.float32, [None, n_classes])\n",
    "# type 1:\n",
    "# cross_entropy = tf.reduce_mean(\n",
    "#     -tf.reduce_sum(y_true * tf.log(y_softmax), reduction_indices=[1]))\n",
    "# type 2:\n",
    "# cross_entropy = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "# type 3:\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=y_pred))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "    cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.为了评估模型所需要输出的数据\n",
    "\n",
    "pred => 预测的分类结果。<br>\n",
    "true => 正确的分类结果。<br>\n",
    "correct_prediction => 对比预测结果和正确结果。<br>\n",
    "accuracy => 正确率。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.argmax(y_pred, 1)\n",
    "true = tf.argmax(y_true, 1)\n",
    "correct_prediction = tf.equal(pred, true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.训练模型并实时输出评估数据\n",
    "\n",
    "循环训练10次，每次训练会随机抓取训练数据中100个数据点。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Batch:    0 \n",
      "Details(0~5)--------------------------------------------\n",
      "train_y_pred: [[-0.11002736  0.18487297  0.44305825 -0.38023573 -0.18479885 -0.2683453\n",
      "   0.25828344  0.05070856 -0.0324292   0.03891305]\n",
      " [-0.30482468  0.19426018  1.136788   -0.6069247  -0.33325022 -0.52722865\n",
      "   0.44119087  0.163528   -0.25636628  0.09282692]\n",
      " [-0.13573974  0.32633132  0.27970797 -0.21609351 -0.121985   -0.16661927\n",
      "   0.06935494  0.02337473 -0.07345591  0.0151243 ]\n",
      " [ 0.04100353 -0.1082223   0.26374707 -0.49610126 -0.24050018 -0.08761284\n",
      "   0.45472875  0.12797147 -0.15274322  0.1977284 ]\n",
      " [-0.25817424  0.09295582  0.636666   -0.6275138  -0.23226711 -0.3915826\n",
      "   0.9120573   0.09065467 -0.26626045  0.04346411]]\n",
      "train_y_softmax: [[0.08705255 0.11691092 0.15135038 0.06644028 0.08078089 0.07430617\n",
      "  0.12581627 0.10223231 0.09407667 0.10103352]\n",
      " [0.0639776  0.10538474 0.27046582 0.04729635 0.06218461 0.05121994\n",
      "  0.134902   0.1021953  0.0671542  0.09521949]\n",
      " [0.08594126 0.13641955 0.1302052  0.07930572 0.08713152 0.08332799\n",
      "  0.10550503 0.10076372 0.09146422 0.0999358 ]\n",
      " [0.10074434 0.08677862 0.12588008 0.05887881 0.07602653 0.08858564\n",
      "  0.15236993 0.10989815 0.0829999  0.11783802]\n",
      " [0.06923134 0.09835503 0.16940527 0.04785206 0.07104836 0.06058487\n",
      "  0.22311452 0.09812896 0.06867377 0.09360577]]\n",
      "train_pred: [2 2 1 6 6]\n",
      "train_true: [8 2 1 5 6]\n",
      "test_pred: [7 2 1 6 2]\n",
      "test_true: [7 2 1 0 4]\n",
      "--------------------------------------------------------\n",
      "Epoch:  0 Batch:  549  train_acc=0.90000 test_acc=0.90940 train_cost=0.442094399\n",
      "Epoch:  1 Batch:  549  train_acc=0.89000 test_acc=0.91920 train_cost=0.325040734\n",
      "Epoch:  2 Batch:  549  train_acc=0.93000 test_acc=0.91800 train_cost=0.305216003\n",
      "Epoch:  3 Batch:  549  train_acc=0.94000 test_acc=0.91710 train_cost=0.295575431\n",
      "Epoch:  4 Batch:  549  train_acc=0.94000 test_acc=0.92220 train_cost=0.288251195\n",
      "Epoch:  5 Batch:  549  train_acc=0.94000 test_acc=0.92110 train_cost=0.283693752\n",
      "Epoch:  6 Batch:  549  train_acc=0.88000 test_acc=0.92380 train_cost=0.279545486\n",
      "Epoch:  7 Batch:  549  train_acc=0.94000 test_acc=0.92360 train_cost=0.276244981\n",
      "Epoch:  8 Batch:  549  train_acc=0.94000 test_acc=0.92360 train_cost=0.273991347\n",
      "Epoch:  9 Batch:    0 \n",
      "Details(0~5)--------------------------------------------\n",
      "train_y_pred: [[ 3.5587728  -5.979827    6.0529957  -1.91834    -0.5772236   4.22892\n",
      "   8.502259   -6.960249    0.45613885 -7.363419  ]\n",
      " [-1.6837807  -4.3148947   1.1275289  -3.018992    4.4415975  -1.7832625\n",
      "   1.2907066  -0.40556216  1.3791826   2.967488  ]\n",
      " [-1.2569709  -3.1436667  -0.3867038   7.2751904  -4.3496585   0.5117016\n",
      "  -7.4005027   3.1624048   1.3831985   4.2050257 ]\n",
      " [-7.603346    9.367674    2.416035    1.9067568  -7.5032268  -0.05585456\n",
      "   0.89322424 -4.1950445   5.4125977  -0.63879883]\n",
      " [-1.7076709  -4.1866655   2.5373604  -0.30240336 -1.8035183  -1.1928532\n",
      "  -2.86547     5.497446    0.1294272   3.8943672 ]]\n",
      "train_y_softmax: [[0.0064354  0.00000046 0.07794763 0.00002691 0.00010288 0.01257815\n",
      "  0.9026192  0.00000017 0.00028915 0.00000012]\n",
      " [0.0015988  0.00011511 0.02659078 0.00042065 0.7311605  0.00144741\n",
      "  0.03130389 0.00574008 0.03419974 0.16742301]\n",
      " [0.00018459 0.00002798 0.00044071 0.93685675 0.00000838 0.00108225\n",
      "  0.0000004  0.01532898 0.0025871  0.04348281]\n",
      " [0.00000004 0.97940755 0.00093736 0.00056329 0.00000005 0.00007914\n",
      "  0.00020444 0.00000126 0.01876272 0.00004418]\n",
      " [0.00058776 0.00004927 0.04100097 0.00239607 0.00053404 0.00098352\n",
      "  0.00018466 0.79130334 0.00369013 0.15927027]]\n",
      "train_pred: [6 4 3 1 7]\n",
      "train_true: [6 4 3 1 7]\n",
      "test_pred: [7 2 1 0 4]\n",
      "test_true: [7 2 1 0 4]\n",
      "--------------------------------------------------------\n",
      "Epoch:  9 Batch:  549  train_acc=0.91000 test_acc=0.92390 train_cost=0.271322928\n",
      "Process Time(s):4.14\n",
      "0.9239\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "start = time.time()\n",
    "for epoch_i in range(training_epochs):\n",
    "    ave_cost = 0\n",
    "    for batch_i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run(\n",
    "            [train_step, cross_entropy],\n",
    "            feed_dict={\n",
    "                x: batch_xs,\n",
    "                y_true: batch_ys\n",
    "            })\n",
    "        ave_cost += c / total_batch\n",
    "        # evidence and softmax data\n",
    "        if (epoch_i == 0 and batch_i == 0) or (epoch_i == 9 and batch_i == 0):\n",
    "            print(\"Epoch:%3d Batch:%5d \" % (epoch_i, batch_i), )\n",
    "            print(\"Details(0~5)--------------------------------------------\")\n",
    "            train_y_pred = y_pred.eval(feed_dict={\n",
    "                x: batch_xs,\n",
    "                y_true: batch_ys\n",
    "            })\n",
    "            train_y_softmax = y_softmax.eval(feed_dict={\n",
    "                x: batch_xs,\n",
    "                y_true: batch_ys\n",
    "            })\n",
    "            print(\"train_y_pred:\", train_y_pred[:5])\n",
    "            print(\"train_y_softmax:\", train_y_softmax[:5])\n",
    "            train_pred = pred.eval(feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "            train_true = true.eval(feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "            test_pred = pred.eval(feed_dict={\n",
    "                x: mnist.test.images,\n",
    "                y_true: mnist.test.labels\n",
    "            })\n",
    "            test_true = true.eval(feed_dict={\n",
    "                x: mnist.test.images,\n",
    "                y_true: mnist.test.labels\n",
    "            })\n",
    "            print(\"train_pred:\", train_pred[:5])\n",
    "            print(\"train_true:\", train_true[:5])\n",
    "            print(\"test_pred:\", test_pred[:5])\n",
    "            print(\"test_true:\", test_true[:5])\n",
    "            print(\"--------------------------------------------------------\")\n",
    "    if epoch_i % 1 == 0:\n",
    "        train_acc = accuracy.eval(feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        test_acc = accuracy.eval(feed_dict={\n",
    "            x: mnist.test.images,\n",
    "            y_true: mnist.test.labels\n",
    "        })\n",
    "        print(\"Epoch:%3d Batch:%5d \" % (epoch_i,\n",
    "                                        batch_i), \"train_acc=%.5f\" % train_acc,\n",
    "              \"test_acc=%.5f\" % test_acc, \"train_cost=%.9f\" % ave_cost)\n",
    "end = time.time()\n",
    "print(\"Process Time(s):{:.2f}\".format(end - start))\n",
    "\n",
    "# result\n",
    "print(\n",
    "    sess.run(\n",
    "        accuracy, feed_dict={\n",
    "            x: mnist.test.images,\n",
    "            y_true: mnist.test.labels\n",
    "        }))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

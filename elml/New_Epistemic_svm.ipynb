{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'on task', 1: 'off task'}\n",
      "old_input_train: 7614\n",
      "old_labels_train: 7614\n",
      "old_input_test: 846\n",
      "old_labels_test: 846\n",
      "new_input: 2743\n",
      "new_labels: 2743\n",
      "new_input[0]: 49 89 0\n",
      "new_labels[1]: [0 0]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "# labelの詳細\n",
    "with open('data/Epistemic_FA_id_to_lb.pickle', mode='rb') as f:\n",
    "    id_to_code = pickle.load(f)\n",
    "    \n",
    "    \n",
    "old_input = np.asarray([\n",
    "    \" \".join(i.strip().split()[2:])\n",
    "    for i in open(\"data/Epistemic_FA_edu_data.txt\").readlines()\n",
    "])\n",
    "old_labels = np.asarray(\n",
    "    [\n",
    "        l.strip().split()[:2]\n",
    "        for l in open(\"data/Epistemic_FA_edu_data.txt\").readlines()\n",
    "    ],\n",
    "    dtype=np.int64)\n",
    "\n",
    "\n",
    "new_input = np.asarray([\n",
    "    \" \".join(i.strip().split()[2:])\n",
    "    for i in open(\"data/New_edu_data.txt\").readlines()\n",
    "])\n",
    "new_labels = np.asarray(\n",
    "    [\n",
    "        l.strip().split()[:2]\n",
    "        for l in open(\"data/New_edu_data.txt\").readlines()\n",
    "    ],\n",
    "    dtype=np.int64)\n",
    "\n",
    "\n",
    "train = np.arange(len(old_input)) % 10 != 0\n",
    "\n",
    "print(id_to_code)\n",
    "print(\"old_input_train:\", len(old_input[train]))\n",
    "print(\"old_labels_train:\", len(old_labels[train]))\n",
    "\n",
    "print(\"old_input_test:\", len(old_input[~train]))\n",
    "print(\"old_labels_test:\", len(old_labels[~train]))\n",
    "\n",
    "print(\"new_input:\", len(new_input))\n",
    "print(\"new_labels:\", len(new_labels))\n",
    "\n",
    "print(\"new_input[0]:\", new_input[0])\n",
    "print(\"new_labels[1]:\", new_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8460, 1)\n",
      "Counter({1: 4543, 0: 3917})\n"
     ]
    }
   ],
   "source": [
    "xxxx = old_labels[:,:1]\n",
    "print(xxxx.shape)\n",
    "print(Counter(np.reshape(xxxx,(-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashingのために、以前のデータとnew_inputを結合する\n",
    "# all_input = [ old_input_train, old_input_test, new_input ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_input_train size: 7614\n",
      "old_input_test size: 846\n",
      "new_input size: 2743\n"
     ]
    }
   ],
   "source": [
    "all_input = np.hstack((old_input[train],old_input[~train],new_input)) \n",
    "a = len(old_input[train])\n",
    "b = len(old_input[~train])\n",
    "c = len(new_input)\n",
    "print(\"old_input_train size:\",a)\n",
    "print(\"old_input_test size:\",b)\n",
    "print(\"new_input size:\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以前のデータの結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pred: Counter({1: 459, 0: 387})  test_true: Counter({1: 456, 0: 390})\n",
      "test acc = 0.9089834515366431\n",
      "{0: 'on task', 1: 'off task'}\n",
      "class: ['on task', 'off task']\n",
      "[[350  37]\n",
      " [ 40 419]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    on task       0.90      0.90      0.90       387\n",
      "   off task       0.92      0.91      0.92       459\n",
      "\n",
      "avg / total       0.91      0.91      0.91       846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat1 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "train_feature = dat1[0:a]\n",
    "train_label = old_labels[train][:,0]\n",
    "test_feature = dat1[a:a+b]\n",
    "test_label = old_labels[~train][:,0]\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "print(\"test_pred:\",Counter(test_pred), \" test_true:\",Counter(test_label))\n",
    "print(\"test acc =\",metrics.accuracy_score(test_label, test_pred))\n",
    "print(id_to_code)\n",
    "print(\"class:\", class_names)\n",
    "print(confusion_matrix(test_pred,test_label))\n",
    "print(classification_report(test_pred,test_label, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新しいデータの予測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1649, 1: 1094})\n",
      "{0: 'on task', 1: 'off task'}\n",
      "[1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1\n",
      " 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0\n",
      " 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
      " 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0\n",
      " 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
      " 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1\n",
      " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
      " 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0\n",
      " 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1\n",
      " 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0\n",
      " 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0\n",
      " 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
      " 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0\n",
      " 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1\n",
      " 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1\n",
      " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1\n",
      " 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1\n",
      " 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1\n",
      " 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0\n",
      " 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1\n",
      " 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
      " 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat2 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "train_feature = dat2[0:a]\n",
    "train_label = old_labels[train][:,0]\n",
    "test_feature = dat2[a:a+b]\n",
    "test_label = old_labels[~train][:,0]\n",
    "test_feature_new = dat2[a+b:a+b+c] #新しいデータ\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "test_new_pred = clf.predict(test_feature_new)\n",
    "print(Counter(test_new_pred))\n",
    "print(id_to_code)\n",
    "print(test_new_pred)\n",
    "\n",
    "\n",
    "of = open(\"data/New_pred_Epistemic_data.txt\", \"w\")\n",
    "for label in test_new_pred:\n",
    "    print(id_to_code[label], file=of)\n",
    "of.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新しいデータの予測結果(train all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1633, 1: 1110})\n",
      "{0: 'on task', 1: 'off task'}\n",
      "[1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1\n",
      " 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
      " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0\n",
      " 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
      " 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0\n",
      " 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
      " 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1\n",
      " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
      " 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0\n",
      " 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1\n",
      " 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0\n",
      " 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
      " 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
      " 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0\n",
      " 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1\n",
      " 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1\n",
      " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1\n",
      " 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
      " 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1\n",
      " 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1\n",
      " 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
      " 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat3 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "# train_feature = dat2[0:a]\n",
    "# train_label = old_labels[train][:,0]\n",
    "# test_feature = dat2[a:a+b]\n",
    "# test_label = old_labels[~train][:,0]\n",
    "\n",
    "train_feature = dat3[0:a+b]\n",
    "\n",
    "train_label = np.hstack((old_labels[train][:,0],old_labels[~train][:,0])) \n",
    "test_feature_new = dat2[a+b:a+b+c] #新しいデータ\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "test_new_pred = clf.predict(test_feature_new)\n",
    "print(Counter(test_new_pred))\n",
    "print(id_to_code)\n",
    "print(test_new_pred)\n",
    "\n",
    "\n",
    "of = open(\"data/New_pred_Epistemic_2_data.txt\", \"w\")\n",
    "for label in test_new_pred:\n",
    "    print(id_to_code[label], file=of)\n",
    "of.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'grounded and qualified claim', 1: 'grounded claim', 2: 'non-argumentative moves', 3: 'simple claim', 4: 'qualified claim'}\n",
      "old_input_train: 6988\n",
      "old_labels_train: 6988\n",
      "old_input_test: 777\n",
      "old_labels_test: 777\n",
      "new_input: 2743\n",
      "new_labels: 2743\n",
      "new_input[0]: 49 89 0\n",
      "new_labels[1]: [0 0]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "# labelの詳細\n",
    "with open('data/Argumentation_FA_id_to_lb.pickle', mode='rb') as f:\n",
    "    id_to_code = pickle.load(f)\n",
    "    \n",
    "    \n",
    "old_input = np.asarray([\n",
    "    \" \".join(i.strip().split()[2:])\n",
    "    for i in open(\"data/Argumentation_FA_edu_data.txt\").readlines()\n",
    "])\n",
    "old_labels = np.asarray(\n",
    "    [\n",
    "        l.strip().split()[:2]\n",
    "        for l in open(\"data/Argumentation_FA_edu_data.txt\").readlines()\n",
    "    ],\n",
    "    dtype=np.int64)\n",
    "\n",
    "\n",
    "new_input = np.asarray([\n",
    "    \" \".join(i.strip().split()[2:])\n",
    "    for i in open(\"data/New_edu_data.txt\").readlines()\n",
    "])\n",
    "new_labels = np.asarray(\n",
    "    [\n",
    "        l.strip().split()[:2]\n",
    "        for l in open(\"data/New_edu_data.txt\").readlines()\n",
    "    ],\n",
    "    dtype=np.int64)\n",
    "\n",
    "\n",
    "train = np.arange(len(old_input)) % 10 != 0\n",
    "\n",
    "print(id_to_code)\n",
    "print(\"old_input_train:\", len(old_input[train]))\n",
    "print(\"old_labels_train:\", len(old_labels[train]))\n",
    "\n",
    "print(\"old_input_test:\", len(old_input[~train]))\n",
    "print(\"old_labels_test:\", len(old_labels[~train]))\n",
    "\n",
    "print(\"new_input:\", len(new_input))\n",
    "print(\"new_labels:\", len(new_labels))\n",
    "\n",
    "print(\"new_input[0]:\", new_input[0])\n",
    "print(\"new_labels[1]:\", new_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7765, 1)\n",
      "Counter({2: 4899, 3: 2622, 1: 228, 4: 13, 0: 3})\n"
     ]
    }
   ],
   "source": [
    "xxxx = old_labels[:,:1]\n",
    "print(xxxx.shape)\n",
    "print(Counter(np.reshape(xxxx,(-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashingのために、以前のデータとnew_inputを結合する\n",
    "# all_input = [ old_input_train, old_input_test, new_input ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_input_train size: 6988\n",
      "old_input_test size: 777\n",
      "new_input size: 2743\n"
     ]
    }
   ],
   "source": [
    "all_input = np.hstack((old_input[train],old_input[~train],new_input)) \n",
    "a = len(old_input[train])\n",
    "b = len(old_input[~train])\n",
    "c = len(new_input)\n",
    "print(\"old_input_train size:\",a)\n",
    "print(\"old_input_test size:\",b)\n",
    "print(\"new_input size:\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以前のデータの結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pred: Counter({2: 499, 3: 267, 1: 11})  test_true: Counter({2: 491, 3: 264, 1: 21, 4: 1})\n",
      "test acc = 0.8828828828828829\n",
      "{0: 'grounded and qualified claim', 1: 'grounded claim', 2: 'non-argumentative moves', 3: 'simple claim', 4: 'qualified claim'}\n",
      "class: ['grounded and qualified claim', 'grounded claim', 'non-argumentative moves', 'simple claim', 'qualified claim']\n",
      "[[  5   1   5   0]\n",
      " [  1 460  38   0]\n",
      " [ 15  30 221   1]\n",
      " [  0   0   0   0]]\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "grounded and qualified claim       0.24      0.45      0.31        11\n",
      "              grounded claim       0.94      0.92      0.93       499\n",
      "     non-argumentative moves       0.84      0.83      0.83       267\n",
      "                simple claim       0.00      0.00      0.00         0\n",
      "\n",
      "                 avg / total       0.89      0.88      0.89       777\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 4, does not match size of target_names, 5\n",
      "  .format(len(labels), len(target_names))\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat1 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "train_feature = dat1[0:a]\n",
    "train_label = old_labels[train][:,0]\n",
    "test_feature = dat1[a:a+b]\n",
    "test_label = old_labels[~train][:,0]\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "print(\"test_pred:\",Counter(test_pred), \" test_true:\",Counter(test_label))\n",
    "print(\"test acc =\",metrics.accuracy_score(test_label, test_pred))\n",
    "print(id_to_code)\n",
    "print(\"class:\", class_names)\n",
    "print(confusion_matrix(test_pred,test_label))\n",
    "print(classification_report(test_pred,test_label, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新しいデータの予測結果1 => 90%の訓練データを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 1649, 3: 1073, 1: 21})\n",
      "{0: 'grounded and qualified claim', 1: 'grounded claim', 2: 'non-argumentative moves', 3: 'simple claim', 4: 'qualified claim'}\n",
      "[2 2 2 2 2 2 2 2 2 2 2 3 2 2 3 3 2 2 1 2 2 2 2 3 3 3 2 3 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 3 3 2 3 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 2 2 2 3 2 3 2 2\n",
      " 2 2 3 2 2 2 3 2 3 2 3 2 2 2 3 2 2 2 2 2 3 2 2 3 2 3 2 2 3 2 2 2 2 2 3 2 3\n",
      " 3 2 3 3 2 2 3 3 2 2 2 2 2 2 2 2 2 3 3 3 2 3 3 3 3 3 3 3 3 3 2 2 3 2 3 2 2\n",
      " 2 2 2 2 2 2 2 3 3 3 3 3 3 2 2 3 3 2 2 2 2 2 2 3 3 3 2 3 3 3 3 2 2 2 2 2 2\n",
      " 2 2 3 3 3 3 2 3 3 3 3 2 3 2 3 3 2 2 2 2 3 3 3 3 2 2 2 2 2 2 3 2 3 3 3 2 3\n",
      " 3 2 3 2 2 3 2 2 2 2 2 2 3 3 3 2 2 2 2 2 3 2 3 2 2 2 2 2 2 2 2 2 2 3 2 3 3\n",
      " 2 3 2 2 2 3 3 3 2 3 2 3 3 3 3 2 3 3 2 3 3 2 3 3 3 2 2 3 3 2 2 2 3 2 3 3 2\n",
      " 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 3 1 3 2 3 2 3\n",
      " 2 3 3 2 3 2 3 2 3 3 3 2 2 2 2 3 3 2 2 3 3 2 2 2 2 2 3 2 2 2 2 2 2 2 3 3 3\n",
      " 2 3 3 2 2 3 3 3 2 3 2 3 3 3 3 2 3 2 2 3 2 2 3 2 2 3 2 2 3 2 2 3 2 3 3 3 2\n",
      " 2 2 2 3 3 2 2 2 2 2 3 2 2 2 3 2 3 2 2 2 2 3 2 1 3 3 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 3 3 2 3 2 2 1 3 3 3 2 2 2 3 3 2 3 2 2 3 3 3 3 2 3 2 3 3 3 3 3 3 3\n",
      " 2 3 2 3 3 2 3 3 3 3 2 3 3 2 3 3 2 3 3 2 3 2 3 3 2 2 3 2 2 2 2 2 2 2 3 3 3\n",
      " 2 3 2 3 3 3 3 2 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 3 2 3 3 2 3 3 3 2 2 2 1 1\n",
      " 3 2 2 2 3 2 2 2 3 3 2 2 2 3 2 2 2 3 2 2 2 2 3 2 2 2 2 2 2 3 3 3 3 3 3 2 3\n",
      " 2 2 2 2 2 2 2 3 2 2 3 3 2 3 3 3 2 2 2 2 2 3 2 2 3 2 2 2 2 3 2 3 2 2 3 2 2\n",
      " 2 2 2 2 2 3 2 2 3 3 2 2 2 2 3 3 2 3 2 2 3 3 3 3 3 2 3 2 2 2 3 2 2 3 3 3 2\n",
      " 2 3 3 3 2 3 3 3 3 2 3 3 2 3 2 2 2 2 3 3 3 3 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 1 3 2 2 2 3 3 2 3\n",
      " 3 2 2 2 1 2 3 2 3 2 2 2 2 2 2 2 2 2 2 3 2 3 2 3 2 3 3 2 3 2 2 2 2 2 3 2 2\n",
      " 2 2 3 3 2 2 2 3 3 2 2 2 2 2 2 2 2 2 3 3 3 2 3 2 2 3 3 3 2 2 3 2 3 3 3 2 2\n",
      " 3 2 2 2 2 2 2 3 2 2 2 2 3 3 3 2 3 3 2 2 2 3 2 2 2 2 2 3 3 2 2 2 2 2 2 3 2\n",
      " 3 2 2 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 3 3 2 2 2 3 2 3 2 2 3 3\n",
      " 3 3 2 2 2 3 3 2 3 3 3 3 3 2 2 2 2 3 2 2 2 2 2 3 3 3 2 3 2 2 2 3 3 3 2 3 2\n",
      " 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 3 2 2 3 3 2 2 2 2 2 3 2 2 2 2 3 2 2 2 2 2 2\n",
      " 2 3 2 3 3 2 3 3 2 3 3 3 3 2 2 2 2 2 2 3 2 3 2 2 2 3 2 3 2 2 3 3 3 2 3 2 3\n",
      " 3 2 2 2 3 2 2 2 2 2 3 3 2 2 3 2 3 2 3 3 2 2 2 2 2 2 3 2 3 3 3 3 3 3 3 2 3\n",
      " 3 2 2 3 2 2 3 3 2 2 3 3 2 2 3 3 3 2 3 3 3 2 2 2 3 3 3 3 2 2 2 2 3 3 3 3 2\n",
      " 2 2 3 2 2 3 2 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 3 3 3 1 1 2 3 3 2 2\n",
      " 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 2 3 3 3 2 2 3 3 3 2 3 2 2 3 2 2 1\n",
      " 2 3 2 2 2 2 3 2 2 2 2 3 3 2 2 2 2 3 2 2 3 2 3 3 2 3 3 2 2 2 2 3 3 3 3 2 3\n",
      " 3 2 3 3 3 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 3 3 2 3 3 2 2 3 3 2 3 3 2 2 2 2 3\n",
      " 2 3 3 2 3 2 2 2 3 2 2 2 3 2 2 3 2 3 2 2 2 3 3 2 3 2 3 3 3 2 2 3 2 2 3 2 2\n",
      " 3 3 2 2 3 3 3 2 2 2 2 2 2 3 2 2 3 2 3 3 3 3 3 3 2 2 3 2 2 2 2 2 2 2 2 2 3\n",
      " 3 2 3 2 2 3 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 2 3 2 3 3 2 2 2 2 3 2 2 2 2 2 2\n",
      " 2 2 2 3 2 3 2 3 2 3 2 2 2 3 2 2 2 2 2 2 2 2 3 2 3 2 3 3 3 3 2 2 2 2 2 2 3\n",
      " 2 3 2 3 2 2 3 2 2 3 3 3 3 3 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3\n",
      " 3 2 3 2 3 3 3 3 2 2 3 2 2 2 2 2 2 3 3 2 3 3 2 2 3 2 2 3 2 2 3 3 3 2 2 2 2\n",
      " 2 2 2 3 2 2 2 3 2 3 3 2 2 2 3 3 2 3 2 3 2 3 2 2 3 3 3 2 2 2 2 2 3 3 2 2 2\n",
      " 2 3 2 2 2 2 3 2 2 3 2 2 2 2 2 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 3 3 3 2 3 3 3\n",
      " 2 3 2 2 3 2 2 3 2 2 3 2 2 3 3 1 3 2 2 2 2 3 3 2 3 2 3 3 2 2 2 3 2 2 3 2 2\n",
      " 2 3 2 2 3 3 3 3 2 3 2 2 2 2 3 3 3 2 2 2 3 3 3 3 2 2 2 3 2 3 3 3 2 2 3 3 3\n",
      " 2 3 2 2 2 3 3 2 2 2 3 2 2 3 2 3 2 2 3 3 2 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 3 2 3 3 3 2 3 3 3 3 2 2 2 3 3 2 2 3 3\n",
      " 2 3 3 3 2 3 3 2 2 3 3 2 2 2 2 2 2 2 3 2 3 2 3 3 3 2 2 2 2 2 2 2 2 2 3 2 2\n",
      " 2 2 2 2 2 2 2 3 2 2 2 3 3 2 2 2 2 3 3 3 3 2 2 3 3 2 2 2 2 2 2 2 2 2 3 2 2\n",
      " 2 3 2 2 2 3 2 3 3 3 2 2 2 2 3 3 3 2 2 2 2 3 3 2 2 2 2 2 3 3 2 3 2 2 2 2 3\n",
      " 2 3 2 2 2 2 3 2 2 3 3 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 2 2 2 2 2 2 2 2 2 2\n",
      " 3 2 2 2 3 2 3 2 3 2 2 2 2 2 3 3 2 2 2 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 3 3 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 2 2 3 2 3 2 3 2 3 2 2 3 3 2 2 2\n",
      " 2 2 3 3 3 3 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 3 3 3 1 2 3 3 3 2 2 2 3 3 3 3\n",
      " 2 2 3 3 2 2 3 2 2 3 2 2 3 3 3 3 2 2 2 3 2 2 2 2 2 3 3 2 3 3 3 3 3 3 2 2 2\n",
      " 2 2 2 3 2 3 3 3 3 1 2 2 2 2 2 3 3 2 3 2 3 3 3 2 3 2 3 3 3 2 2 2 2 2 2 3 2\n",
      " 3 3 2 3 3 2 3 3 3 3 3 3 2 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 3 2 3 3 3 2 3 2 2 3 2 2 2 2 2 2 2 3 3 2 2 3 2 3 3 2 3 2 2 3 2 3\n",
      " 2 2 2 2 3 3 3 3 3 2 2 3 2 2 3 2 2 2 3 2 2 2 2 2 2 2 3 3 3 2 2 2 3 2 2 2 2\n",
      " 2 3 2 2 2 3 3 3 2 2 2 3 2 2 3 3 2 3 3 2 2 3 3 3 3 3 2 2 2 2 2 3 3 3 2 2 2\n",
      " 2 3 2 2 2 2 3 2 2 3 2 2 2 3 2 2 2 2 3 3 3 2 2 2 2 3 3 3 2 3 3 3 3 3 3 3 3\n",
      " 3 3 2 2 2 2 2 2 2 2 3 3 3 1 3 3 2 2 3 3 3 3 3 2 2 3 3 3 2 3 2 1 3 3 3 3 2\n",
      " 2 2 2 3 3 3 2 2 2 2 2 3 3 3 3 3 2 3 2 3 2 2 2 2 3 2 3 3 3 3 3 3 2 2 2 2 3\n",
      " 3 2 2 3 2 2 2 3 2 2 2 3 2 2 2 3 2 2 3 3 3 2 2 3 3 2 3 2 3 2 3 3 2 3 3 3 3\n",
      " 2 3 1 1 2 3 3 2 3 3 2 2 3 3 2 3 3 3 3 3 3 3 2 2 3 3 2 3 3 2 2 2 3 3 2 2 2\n",
      " 2 2 3 2 3 2 2 3 2 3 3 3 3 2 2 3 3 3 2 2 2 2 2 2 3 3 3 2 2 2 3 2 2 3 3 2 3\n",
      " 2 3 2 2 2 2 2 2 2 2 2 3 2 2 2 3 2 2 3 2 2 3 3 2 2 2 2 2 2 3 2 3 2 3 3 3 3\n",
      " 3 3 2 3 2 3 3 2 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 3\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 3 3 2 3 2 3 3 3 3 3 2 2 2 2 2 2 3 3 3 2 2 2\n",
      " 2 2 2 2 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 2 2 2 3 2 3 2 2 2 2 2 2 3\n",
      " 2 2 2 2 2 2 2 2 2 3 3 2 2 2 2 3 2 2 3 2 2 2 2 2 2 3 2 2 2 3 2 2 2 2 3 2 3\n",
      " 3 2 2 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3 3 3 2 3 2 2 2 2\n",
      " 2 2 3 2 2 3 2 2 3 2 2 2 2 3 3 3 2 3 3 3 3 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2\n",
      " 2 3 3 3 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 2 2 3\n",
      " 3 2 3 3 3 3 3 3 3 3 3 2 2 2 2 3 2 2 2 2 2 2 3 2 3 3 3 2 2 3 3 2 3 2 2 2 2\n",
      " 3 3 2 2 2 3 2 2 2 2 2 2 3 3 2 2 3 2 3 2 2 3 3 3 3 2 2 3 3 3 3 3 3 2 2 2 2\n",
      " 2 2 3 2 2]\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat2 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "train_feature = dat2[0:a]\n",
    "train_label = old_labels[train][:,0]\n",
    "test_feature = dat2[a:a+b]\n",
    "test_label = old_labels[~train][:,0]\n",
    "test_feature_new = dat2[a+b:a+b+c] #新しいデータ\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "test_new_pred = clf.predict(test_feature_new)\n",
    "print(Counter(test_new_pred))\n",
    "print(id_to_code)\n",
    "print(test_new_pred)\n",
    "\n",
    "\n",
    "of = open(\"data/New_pred_Argumentation_data_1.txt\", \"w\")\n",
    "for label in test_new_pred:\n",
    "    print(id_to_code[label], file=of)\n",
    "of.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新しいデータの予測結果2 => 100%の訓練データを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 1638, 3: 1082, 1: 23})\n",
      "{0: 'grounded and qualified claim', 1: 'grounded claim', 2: 'non-argumentative moves', 3: 'simple claim', 4: 'qualified claim'}\n",
      "[2 2 2 2 2 2 2 2 2 2 2 3 2 2 3 3 2 2 1 2 2 2 2 3 3 3 2 3 1 2 2 3 2 2 2 2 2\n",
      " 2 2 2 2 2 2 3 3 2 3 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 2 2 2 3 2 3 2 2\n",
      " 2 2 2 2 2 2 3 2 3 2 3 2 2 2 3 2 2 2 2 2 3 2 2 3 2 3 2 2 3 2 2 2 2 2 3 2 3\n",
      " 3 2 3 3 2 2 3 3 2 2 2 2 2 2 2 2 2 3 3 3 2 3 3 3 3 3 3 3 3 3 2 2 2 2 3 2 2\n",
      " 2 2 2 2 2 2 2 3 3 3 3 3 3 2 2 3 3 2 2 2 2 2 2 3 3 3 2 3 3 3 3 2 2 2 2 2 2\n",
      " 2 2 3 3 3 3 2 3 3 3 3 3 3 2 3 3 2 2 2 2 3 3 3 3 2 2 2 2 2 2 3 2 3 2 3 2 3\n",
      " 3 2 3 2 2 3 2 2 2 2 2 2 3 3 3 2 2 2 2 2 3 2 3 2 2 2 2 2 2 2 2 2 2 3 2 3 3\n",
      " 2 3 2 2 2 3 3 3 2 3 2 3 3 3 3 2 3 3 2 1 3 2 3 3 3 2 2 3 3 2 2 2 3 2 3 3 2\n",
      " 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 1 3 2 3 2 3\n",
      " 2 3 3 3 3 3 3 2 3 3 3 2 2 2 2 3 3 2 2 3 3 2 2 2 2 2 3 2 2 2 2 2 2 2 3 3 3\n",
      " 2 3 3 2 2 3 3 3 2 3 2 3 3 3 2 2 3 2 2 3 2 2 3 3 2 3 2 2 3 2 2 3 2 3 3 3 2\n",
      " 2 2 2 3 3 2 2 2 2 2 3 2 2 2 3 2 3 2 2 2 3 3 2 1 3 3 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 3 3 2 3 2 2 1 3 3 3 2 2 2 3 3 2 3 2 2 3 3 3 3 3 3 2 3 3 3 3 3 3 3\n",
      " 2 3 2 3 3 2 3 3 3 3 2 3 3 2 3 3 2 3 3 2 3 2 3 3 3 2 3 2 2 2 2 2 2 2 3 3 3\n",
      " 2 3 2 3 3 3 3 2 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 3 2 3 3 2 3 3 3 2 2 2 1 1\n",
      " 3 2 2 2 3 3 2 2 3 3 2 2 2 3 2 2 2 3 2 2 3 2 3 2 2 2 3 2 2 3 3 3 3 3 3 2 3\n",
      " 2 2 2 2 2 2 2 3 2 2 3 3 2 3 3 3 2 2 2 2 2 3 2 2 3 2 2 2 2 3 2 3 2 2 3 2 2\n",
      " 2 2 2 2 2 3 2 2 3 3 2 2 2 2 3 3 2 3 2 2 3 3 3 3 3 2 3 2 2 2 3 2 2 3 3 3 2\n",
      " 2 3 3 3 2 3 3 3 3 2 3 3 2 3 2 2 2 2 3 3 3 2 3 2 3 2 2 2 2 2 2 2 2 2 3 2 2\n",
      " 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 1 3 2 2 2 3 3 2 3\n",
      " 3 2 2 2 1 3 3 2 3 2 2 2 2 2 2 2 2 2 2 3 2 3 2 2 2 3 3 2 3 2 2 2 2 2 3 2 2\n",
      " 2 2 3 3 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 3 3 2 3 2 2 3 3 3 2 2 3 2 3 3 3 2 2\n",
      " 3 2 2 2 2 2 2 3 2 2 2 2 3 3 3 2 3 3 2 2 2 3 2 2 2 2 2 3 3 2 2 2 2 2 2 3 2\n",
      " 3 2 2 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 3 3 2 2 2 3 2 3 2 2 3 3\n",
      " 3 3 2 2 2 3 3 2 3 3 3 3 3 2 2 2 2 3 2 2 2 2 2 3 3 3 2 3 2 2 2 3 3 3 2 3 3\n",
      " 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 3 2 2 3 3 2 2 2 2 2 3 2 2 2 2 3 2 2 2 2 2 2\n",
      " 2 3 2 3 3 2 3 3 2 3 3 3 3 2 2 2 2 2 2 3 3 3 2 2 2 3 2 3 2 2 3 3 3 2 3 2 3\n",
      " 3 2 2 2 3 2 2 2 2 2 3 3 2 2 2 2 3 2 3 3 2 2 2 2 2 2 3 2 3 3 3 3 3 3 3 2 3\n",
      " 3 2 2 3 2 2 3 3 2 2 3 3 2 2 3 3 3 2 3 3 3 2 2 2 3 3 3 3 2 2 2 2 3 3 3 3 2\n",
      " 2 2 3 2 2 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 3 3 3 3 1 1 2 3 3 2 2\n",
      " 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 2 3 3 3 2 2 3 3 3 2 3 2 2 3 2 2 1\n",
      " 2 3 3 2 2 2 3 2 2 2 2 3 3 2 2 2 2 3 3 2 3 2 3 3 2 3 3 2 2 2 2 3 3 3 3 2 3\n",
      " 3 2 3 3 3 3 2 2 2 2 2 2 2 2 3 2 2 2 2 3 3 3 2 3 3 2 2 2 3 2 3 3 2 2 2 2 3\n",
      " 2 3 3 2 3 2 2 2 3 2 2 2 3 2 2 3 2 2 2 2 2 2 3 2 3 2 3 3 3 2 2 3 2 3 3 2 2\n",
      " 3 3 2 2 3 3 3 2 2 2 2 2 2 3 2 2 3 2 3 3 2 3 3 3 2 2 3 2 2 2 2 2 2 2 2 2 2\n",
      " 3 2 3 2 2 3 2 3 2 3 2 2 2 2 2 2 2 2 3 3 3 2 3 2 3 3 2 2 2 2 3 2 2 2 2 2 2\n",
      " 2 2 2 2 2 3 2 3 2 3 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 2 3 3 3 3 2 2 2 2 2 2 3\n",
      " 2 3 2 3 2 2 3 2 2 3 3 3 3 3 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3\n",
      " 3 2 3 2 3 3 3 3 2 2 3 2 2 2 2 2 2 3 3 2 3 3 2 2 3 2 2 3 2 2 3 3 3 2 2 2 2\n",
      " 2 2 2 3 2 2 2 3 2 3 3 2 2 2 3 3 2 3 2 3 2 3 2 2 3 3 3 2 2 2 2 2 3 3 2 2 2\n",
      " 2 3 2 2 2 2 3 3 2 3 2 2 2 2 2 3 2 2 2 3 2 2 2 3 3 2 2 2 2 2 3 3 3 2 3 3 3\n",
      " 2 3 2 2 3 2 2 3 2 2 3 2 2 3 3 1 3 3 2 2 2 3 3 2 3 2 3 3 2 2 2 3 2 2 3 2 2\n",
      " 2 3 2 2 3 3 3 3 2 3 2 2 2 2 3 3 3 2 2 2 3 3 3 3 2 2 2 3 2 3 3 3 2 2 2 3 3\n",
      " 2 3 2 2 2 3 3 2 2 2 3 2 2 3 2 3 2 2 2 3 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3 3 3 2 3 3 3 3 2 2 2 3 3 2 2 3 3\n",
      " 2 3 3 3 2 3 3 2 2 3 3 2 2 2 2 2 2 2 3 2 3 2 3 3 3 2 2 2 2 2 2 2 2 2 3 2 2\n",
      " 2 2 2 2 2 2 2 3 2 2 2 3 3 2 2 2 2 3 3 3 3 2 2 3 3 2 2 2 2 2 2 2 2 2 3 2 2\n",
      " 2 3 2 2 2 3 2 3 3 3 3 3 2 2 3 3 3 2 2 2 2 3 3 2 2 2 2 2 3 3 2 3 2 2 2 2 3\n",
      " 2 3 2 2 2 2 3 2 2 3 3 3 2 3 3 3 2 3 3 3 3 2 3 3 3 2 3 2 2 2 2 2 2 2 2 2 2\n",
      " 3 2 2 2 3 2 3 2 3 2 2 2 2 2 1 3 2 3 2 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 3 3 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2 2 2 2 2 3 2 3 2 3 2 3 2 2 3 3 2 2 2\n",
      " 2 2 3 3 3 3 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2 3 3 3 3 1 2 3 3 3 2 2 2 3 3 3 3\n",
      " 2 2 3 3 2 2 3 2 2 3 2 2 3 3 3 3 2 2 2 3 2 2 2 2 2 3 3 2 3 3 3 3 3 3 3 2 2\n",
      " 2 2 2 3 2 3 3 3 3 1 2 2 2 2 2 3 3 2 3 3 3 3 3 2 3 2 2 2 2 2 2 2 2 2 2 3 2\n",
      " 3 3 2 3 3 2 3 3 3 3 3 3 2 3 3 3 2 2 2 3 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 3 2 3 3 3 2 3 2 2 3 2 2 2 2 2 2 2 3 3 2 2 3 2 3 3 2 3 3 2 3 2 3\n",
      " 2 2 2 2 3 3 3 3 3 2 2 3 2 2 3 2 2 2 3 2 2 2 2 2 2 2 3 3 3 2 2 2 3 2 2 2 2\n",
      " 2 3 2 2 2 3 3 3 2 2 2 3 2 2 3 3 2 3 3 2 2 3 3 3 3 3 2 2 2 2 2 3 3 3 2 2 2\n",
      " 2 3 2 2 2 2 3 2 2 3 2 2 2 3 2 2 2 2 3 3 3 2 2 2 2 3 3 3 2 3 3 3 3 3 3 3 3\n",
      " 3 3 2 2 2 2 2 2 2 2 3 3 3 1 3 3 2 2 3 3 3 3 3 2 2 3 3 3 2 3 2 1 3 3 3 3 2\n",
      " 2 2 2 3 3 3 2 2 2 2 2 3 3 3 3 3 2 3 2 3 2 2 2 2 3 2 3 3 3 3 3 3 2 2 2 2 3\n",
      " 3 2 2 3 2 2 2 3 2 2 2 3 2 2 2 3 2 2 3 3 3 2 2 3 3 2 3 2 3 2 3 3 2 3 3 3 3\n",
      " 2 3 1 1 2 3 3 2 3 3 2 2 3 3 2 3 3 3 3 3 3 3 2 2 3 3 2 3 3 2 2 2 3 3 2 2 2\n",
      " 2 2 3 2 3 2 2 3 2 3 3 3 3 2 2 3 3 3 2 2 2 2 2 2 3 3 3 2 2 2 3 2 2 3 3 2 3\n",
      " 2 3 2 2 2 2 2 2 2 2 2 3 2 2 2 3 2 2 3 2 2 3 3 2 2 2 2 2 2 3 2 3 2 3 3 3 3\n",
      " 3 3 2 3 2 3 3 2 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 3\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 3 3 2 3 2 3 3 3 3 3 2 2 2 2 2 2 3 3 3 2 2 2\n",
      " 2 2 2 2 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 2 2 2 3 3 2 2 2 2 2 2 2 3\n",
      " 2 2 2 2 2 2 2 2 2 3 3 2 2 2 2 3 2 2 3 2 2 2 2 2 2 3 2 2 2 3 2 2 2 2 3 2 3\n",
      " 3 2 2 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3 3 3 2 3 2 2 2 2\n",
      " 2 3 3 3 3 3 2 2 3 2 2 2 2 3 3 3 2 3 3 3 3 2 3 2 2 2 2 2 2 2 3 3 3 3 2 3 2\n",
      " 2 3 3 3 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 2 3 3\n",
      " 3 2 3 3 3 3 3 3 3 3 3 2 2 2 2 3 2 2 2 2 2 2 3 2 3 3 3 2 2 3 3 2 3 2 2 2 2\n",
      " 3 3 2 2 2 3 2 2 2 2 2 2 3 3 2 2 3 2 2 2 2 3 3 3 3 2 2 3 3 3 3 3 3 2 2 2 2\n",
      " 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat3 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "# train_feature = dat2[0:a]\n",
    "# train_label = old_labels[train][:,0]\n",
    "# test_feature = dat2[a:a+b]\n",
    "# test_label = old_labels[~train][:,0]\n",
    "\n",
    "train_feature = dat3[0:a+b]\n",
    "\n",
    "train_label = np.hstack((old_labels[train][:,0],old_labels[~train][:,0])) \n",
    "test_feature_new = dat2[a+b:a+b+c] #新しいデータ\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "test_new_pred = clf.predict(test_feature_new)\n",
    "print(Counter(test_new_pred))\n",
    "print(id_to_code)\n",
    "print(test_new_pred)\n",
    "\n",
    "\n",
    "of = open(\"data/New_pred_Argumentation_data_2.txt\", \"w\")\n",
    "for label in test_new_pred:\n",
    "    print(id_to_code[label], file=of)\n",
    "of.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'time management', 1: 'task division', 2: 'proceedings', 3: 'technical coordination', 4: 'blank', 5: 'quote'}\n",
      "old_input_train: 3159\n",
      "old_labels_train: 3159\n",
      "old_input_test: 351\n",
      "old_labels_test: 351\n",
      "new_input: 2743\n",
      "new_labels: 2743\n",
      "new_input[0]: 49 89 0\n",
      "new_labels[1]: [0 0]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "# labelの詳細\n",
    "with open('data/Coordination_FA_id_to_lb.pickle', mode='rb') as f:\n",
    "    id_to_code = pickle.load(f)\n",
    "    \n",
    "    \n",
    "old_input = np.asarray([\n",
    "    \" \".join(i.strip().split()[2:])\n",
    "    for i in open(\"data/Coordination_FA_edu_data.txt\").readlines()\n",
    "])\n",
    "old_labels = np.asarray(\n",
    "    [\n",
    "        l.strip().split()[:2]\n",
    "        for l in open(\"data/Coordination_FA_edu_data.txt\").readlines()\n",
    "    ],\n",
    "    dtype=np.int64)\n",
    "\n",
    "\n",
    "new_input = np.asarray([\n",
    "    \" \".join(i.strip().split()[2:])\n",
    "    for i in open(\"data/New_edu_data.txt\").readlines()\n",
    "])\n",
    "new_labels = np.asarray(\n",
    "    [\n",
    "        l.strip().split()[:2]\n",
    "        for l in open(\"data/New_edu_data.txt\").readlines()\n",
    "    ],\n",
    "    dtype=np.int64)\n",
    "\n",
    "\n",
    "train = np.arange(len(old_input)) % 10 != 0\n",
    "\n",
    "print(id_to_code)\n",
    "print(\"old_input_train:\", len(old_input[train]))\n",
    "print(\"old_labels_train:\", len(old_labels[train]))\n",
    "\n",
    "print(\"old_input_test:\", len(old_input[~train]))\n",
    "print(\"old_labels_test:\", len(old_labels[~train]))\n",
    "\n",
    "print(\"new_input:\", len(new_input))\n",
    "print(\"new_labels:\", len(new_labels))\n",
    "\n",
    "print(\"new_input[0]:\", new_input[0])\n",
    "print(\"new_labels[1]:\", new_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3510, 1)\n",
      "Counter({4: 2502, 3: 712, 2: 227, 0: 36, 5: 21, 1: 12})\n"
     ]
    }
   ],
   "source": [
    "xxxx = old_labels[:,:1]\n",
    "print(xxxx.shape)\n",
    "print(Counter(np.reshape(xxxx,(-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashingのために、以前のデータとnew_inputを結合する\n",
    "# all_input = [ old_input_train, old_input_test, new_input ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_input_train size: 3159\n",
      "old_input_test size: 351\n",
      "new_input size: 2743\n"
     ]
    }
   ],
   "source": [
    "all_input = np.hstack((old_input[train],old_input[~train],new_input)) \n",
    "a = len(old_input[train])\n",
    "b = len(old_input[~train])\n",
    "c = len(new_input)\n",
    "print(\"old_input_train size:\",a)\n",
    "print(\"old_input_test size:\",b)\n",
    "print(\"new_input size:\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以前のデータの結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pred: Counter({4: 263, 3: 70, 2: 16, 0: 2})  test_true: Counter({4: 242, 3: 82, 2: 20, 0: 4, 1: 2, 5: 1})\n",
      "test acc = 0.8660968660968661\n",
      "{0: 'time management', 1: 'task division', 2: 'proceedings', 3: 'technical coordination', 4: 'blank', 5: 'quote'}\n",
      "class: ['time management', 'task division', 'proceedings', 'technical coordination', 'blank', 'quote']\n",
      "[[  1   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0  12   1   3   0]\n",
      " [  0   2   3  59   6   0]\n",
      " [  3   0   5  22 232   1]\n",
      " [  0   0   0   0   0   0]]\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "       time management       0.25      0.50      0.33         2\n",
      "         task division       0.00      0.00      0.00         0\n",
      "           proceedings       0.60      0.75      0.67        16\n",
      "technical coordination       0.72      0.84      0.78        70\n",
      "                 blank       0.96      0.88      0.92       263\n",
      "                 quote       0.00      0.00      0.00         0\n",
      "\n",
      "           avg / total       0.89      0.87      0.88       351\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat1 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "train_feature = dat1[0:a]\n",
    "train_label = old_labels[train][:,0]\n",
    "test_feature = dat1[a:a+b]\n",
    "test_label = old_labels[~train][:,0]\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "print(\"test_pred:\",Counter(test_pred), \" test_true:\",Counter(test_label))\n",
    "print(\"test acc =\",metrics.accuracy_score(test_label, test_pred))\n",
    "print(id_to_code)\n",
    "print(\"class:\", class_names)\n",
    "print(confusion_matrix(test_pred,test_label))\n",
    "print(classification_report(test_pred,test_label, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新しいデータの予測結果1 => 90%の訓練データを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat2 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "train_feature = dat2[0:a]\n",
    "train_label = old_labels[train][:,0]\n",
    "test_feature = dat2[a:a+b]\n",
    "test_label = old_labels[~train][:,0]\n",
    "test_feature_new = dat2[a+b:a+b+c] #新しいデータ\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "test_new_pred = clf.predict(test_feature_new)\n",
    "print(Counter(test_new_pred))\n",
    "print(id_to_code)\n",
    "print(test_new_pred)\n",
    "\n",
    "\n",
    "of = open(\"data/New_pred_Coordination_data_1.txt\", \"w\")\n",
    "for label in test_new_pred:\n",
    "    print(id_to_code[label], file=of)\n",
    "of.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新しいデータの予測結果2 => 100%の訓練データを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "h_vectorizer = HashingVectorizer(non_negative=True, ngram_range=(1, 2), norm=u'l2')\n",
    "dat3 = h_vectorizer.fit_transform(all_input.tolist())\n",
    "\n",
    "# train_feature = dat2[0:a]\n",
    "# train_label = old_labels[train][:,0]\n",
    "# test_feature = dat2[a:a+b]\n",
    "# test_label = old_labels[~train][:,0]\n",
    "\n",
    "train_feature = dat3[0:a+b]\n",
    "\n",
    "train_label = np.hstack((old_labels[train][:,0],old_labels[~train][:,0])) \n",
    "test_feature_new = dat2[a+b:a+b+c] #新しいデータ\n",
    "class_names = list(id_to_code.values())\n",
    "\n",
    "clf = svm.SVC(kernel='linear',probability=True)\n",
    "clf.fit(train_feature,train_label)\n",
    "test_pred = clf.predict(test_feature)\n",
    "test_new_pred = clf.predict(test_feature_new)\n",
    "print(Counter(test_new_pred))\n",
    "print(id_to_code)\n",
    "print(test_new_pred)\n",
    "\n",
    "\n",
    "of = open(\"data/New_pred_Coordination_data_2.txt\", \"w\")\n",
    "for label in test_new_pred:\n",
    "    print(id_to_code[label], file=of)\n",
    "of.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

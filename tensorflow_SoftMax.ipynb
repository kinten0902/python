{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "softmax模型可以用来给不同的对象分配概率。<br>\n",
    "在训练更加复杂的模型时，最后一步也往往需要用softmax来分配概率。<br>\n",
    "\n",
    "softmax回归分两步：\n",
    "* 首先 对某个待分类对象属于某个类的“证据”相加求和\n",
    "* 然后 将这个“证据”的和和转化为概率\n",
    "\n",
    "例如计算一张图片是否属于某类。<br>\n",
    "使用加权的方法来累积计算这张图是否属于某类的“证据”。<br>\n",
    "如果图片的像素强有力的体现该图不属于某个类，则权重的数值为负数。<br>\n",
    "相反，则权重的数值为正。<br>\n",
    "还需要引入额外的“证据”，称之为偏置量(bias)<br>\n",
    "因此对于给定的输入图片$x$是属于第$i$类的总体“证据”可以表示为：\n",
    "\n",
    "$$\n",
    "evidence_i = \\sum_{i}W_{i,j}x_j + b_i\n",
    "$$\n",
    "\n",
    "其中$W_i$代表权重，$b_i$代表第$i$类的偏置量，$j$代表给定图片$x$的像素索引 用于求和<br>\n",
    "然后利用softmax函数可以把这些“证据”转化成概率$y$\n",
    "\n",
    "$$\n",
    "y = softmax(evidence)\n",
    "$$\n",
    "\n",
    "给定一张图$x$，它对于每一个类别的吻合度可以被softmax函数转换成一个概率值<br>\n",
    "softmax函数可以定义为：\n",
    "\n",
    "$$\n",
    "softmax(x) = normalize(exp(x))\n",
    "$$\n",
    "\n",
    "展开右边可以得到：\n",
    "\n",
    "$$\n",
    "softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j}exp(x_j)}\n",
    "$$\n",
    "\n",
    "假设模型里的权值不可以是0或是负数，softmax会正则化这些权重值，<br>\n",
    "是它们的总和等于1，以此构建一个有效率的概率分布。\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3\n",
    "\\end{matrix}\n",
    "\\right] = softmax\\left(\n",
    "\\begin{matrix}\n",
    "W_{1,1}x_1 + W_{1,2}x_1 + W_{1,3}x_1 + b_1 \\\\\n",
    "W_{2,1}x_2 + W_{2,2}x_2 + W_{2,3}x_2 + b_2 \\\\\n",
    "W_{3,1}x_3 + W_{3,2}x_3 + W_{3,3}x_3 + b_3\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "转换成用 矩阵乘法和向量相加来表示：\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3\n",
    "\\end{matrix}\n",
    "\\right] = softmax\\left(\n",
    "\\left[\\begin{matrix}\n",
    "W_{1,1}&W_{1,2}&W_{1,3} \\\\\n",
    "W_{2,1}&W_{2,2}&W_{2,3} \\\\\n",
    "W_{3,1}&W_{3,2}&W_{3,3}\n",
    "\\end{matrix}\\right]\n",
    "\\cdot\n",
    "\\left[\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{matrix}\\right]\n",
    "+\n",
    "\\left[\\begin{matrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3\n",
    "\\end{matrix}\\right]\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "简化为：\n",
    "$$\n",
    "y = softmax(W_x+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型\n",
    "为了训练模型，通常需要定义一个指标来评估模型的好坏。<br>\n",
    "这个指标被称为成本(cost)或是损失(loss)，两者意思相同，然后尽量最小化这个指标。<br>\n",
    "非常常见的成本函数是“交叉熵”(cross-entropy):\n",
    "\n",
    "$$\n",
    "H_{y'}(y) = -\\sum_{i}y'_i\\log(y_i)\n",
    "$$\n",
    "\n",
    "其中y是预测的概率分布。<br>\n",
    "y‘是实际的分布。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算交叉熵\n",
    "（重要）<br>\n",
    "交叉熵不仅仅用来衡量单一的一对预测和真实值，<br>\n",
    "而是所有图片的交叉熵的总和。<br>\n",
    "对100个数据点的预测的表示比单一数据点的预测的表示能更好的描述模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_evidence: [[ 0.32188052 -0.3904405  -0.28692713  0.05285639 -0.16995746  0.41891274\n",
      "  -0.19985893 -0.01741308  0.47669202 -0.20574495]\n",
      " [ 1.6122698  -0.7126472  -0.33322763  0.26110667 -0.5269781   0.9752624\n",
      "  -0.20170833 -0.66284126  0.23418725 -0.64542395]\n",
      " [ 1.2996368  -0.66970944 -0.29076073  0.3634644  -0.5076072   0.6981348\n",
      "  -0.2402843  -0.5227661   0.37200448 -0.5021132 ]\n",
      " [ 0.01208    -0.03348467 -0.08073807  0.25594392 -0.37128705  0.43161145\n",
      "  -0.16212702 -0.14578325  0.41461423 -0.32082972]\n",
      " [ 0.43959597 -0.2618383  -0.21195993  0.11411421 -0.36798555  0.7507462\n",
      "  -0.22591457 -0.23466413  0.31780407 -0.31989822]]\n",
      "train_softmax: [[0.13200922 0.0647511  0.07181289 0.10087151 0.08072382 0.14546041\n",
      "  0.07834578 0.09402664 0.15411256 0.07788601]\n",
      " [0.36117965 0.03532033 0.05161839 0.09352345 0.04252651 0.19101807\n",
      "  0.05887387 0.03712403 0.09103946 0.03777629]\n",
      " [0.29549554 0.04123583 0.06023516 0.11587143 0.04849253 0.16192798\n",
      "  0.06335366 0.04776298 0.11686523 0.04875968]\n",
      " [0.09753876 0.09319418 0.08889287 0.1244761  0.06647878 0.14838071\n",
      "  0.08194456 0.08329484 0.14587997 0.0699192 ]\n",
      " [0.1443627  0.07158566 0.07524677 0.10425577 0.06437642 0.19705427\n",
      "  0.07420403 0.0735576  0.127809   0.06754775]]\n",
      "train_pred: [8 0 0 5 5]\n",
      "train_true: [7 0 0 7 5]\n",
      "test_pred: [8 5 5 0 8]\n",
      "test_true: [7 2 1 0 4]\n",
      "Epoch:  0 Batch:  549  train_acc=0.92000 test_acc=0.91060 train_cost=0.442648691\n",
      "Epoch:  1 Batch:  549  train_acc=0.95000 test_acc=0.91510 train_cost=0.325257522\n",
      "Epoch:  2 Batch:  549  train_acc=0.96000 test_acc=0.91960 train_cost=0.305335319\n",
      "Epoch:  3 Batch:  549  train_acc=0.95000 test_acc=0.91900 train_cost=0.295373014\n",
      "Epoch:  4 Batch:  549  train_acc=0.94000 test_acc=0.92220 train_cost=0.288627415\n",
      "Epoch:  5 Batch:  549  train_acc=0.96000 test_acc=0.92120 train_cost=0.283427602\n",
      "Epoch:  6 Batch:  549  train_acc=0.95000 test_acc=0.92390 train_cost=0.279180362\n",
      "Epoch:  7 Batch:  549  train_acc=0.96000 test_acc=0.91780 train_cost=0.276378482\n",
      "Epoch:  8 Batch:  549  train_acc=0.95000 test_acc=0.92200 train_cost=0.273857679\n",
      "train_evidence: [[ -6.1860995    6.1374173    1.4292817    0.06294897  -0.01136327\n",
      "   -1.506573    -0.9557072    1.3668752    0.2461307   -0.58290684]\n",
      " [ -0.17226341  -4.1114726    2.7964163   -0.81142366  -0.7786145\n",
      "    4.4268465   11.703603   -14.76696      4.8722105   -3.158314  ]\n",
      " [ -1.0951314  -13.341471     2.6736147   -1.3219203    9.892926\n",
      "   -2.7597935   -0.62698233  -2.2249875    2.4418576    6.3619084 ]\n",
      " [ -7.4099374   -3.9446726   -4.714194    -2.1473894    8.846956\n",
      "   -0.1653881   -1.5306748    0.74648786   3.7081196    6.6107244 ]\n",
      " [ 10.24379     -9.694048     1.4222591    0.629166    -5.6590433\n",
      "    2.0354083   -3.4539397    1.3627758    1.2019083    1.9117433 ]]\n",
      "train_softmax: [[0.00000433 0.97350323 0.00878254 0.0022399  0.00207948 0.00046622\n",
      "  0.00080878 0.0082512  0.0026902  0.00117419]\n",
      " [0.00000694 0.00000014 0.00013515 0.00000366 0.00000379 0.0006901\n",
      "  0.99808264 0.         0.00107728 0.00000035]\n",
      " [0.0000164  0.         0.00071053 0.00001307 0.9702574  0.0000031\n",
      "  0.00002619 0.0000053  0.00056355 0.02840439]\n",
      " [0.00000008 0.0000025  0.00000116 0.00001509 0.8983093  0.0001095\n",
      "  0.00002796 0.00027254 0.00526814 0.09599371]\n",
      " [0.9990151  0.         0.00014738 0.00006668 0.00000012 0.00027209\n",
      "  0.00000112 0.00013887 0.00011823 0.00024044]]\n",
      "train_pred: [1 6 4 4 0]\n",
      "train_true: [1 6 4 4 0]\n",
      "test_pred: [7 2 1 0 4]\n",
      "test_true: [7 2 1 0 4]\n",
      "Epoch:  9 Batch:  549  train_acc=0.96000 test_acc=0.92090 train_cost=0.271285387\n",
      "Process Time(s):5.43\n",
      "0.9209\n"
     ]
    }
   ],
   "source": [
    "### 参数设置----------------------------------\n",
    "\n",
    "# MNIST数据的图片特征个数 28x28=784\n",
    "n_input = 784\n",
    "\n",
    "# MNIST数据的种类个数 0-9\n",
    "n_classes = 10\n",
    "\n",
    "# 学习速率，比较好的策略是先设置为0.25\n",
    "# 然后在训练到第20个Epoch时改为0.025\n",
    "# 学习速率太大时会导致代价函数振荡\n",
    "# 学习速率太小时会导致收敛过慢\n",
    "learning_rate = 0.25\n",
    "\n",
    "# 批尺寸mini-batch\n",
    "# 太大，权重的更新就不会那么频繁，优化过程太漫长\n",
    "# 太小，计算的加速效果越不明显\n",
    "batch_size = 100\n",
    "\n",
    "# 训练数据55000个，每次取100，学习一个epoch需要550次\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# 单次训练迭代\n",
    "# 1个epoch意味着训练数据被用过一遍\n",
    "training_epochs = 10\n",
    "\n",
    "### 创建模型----------------------------------\n",
    "\n",
    "# 创建一个占位符x来保存输入值\n",
    "# None表示第一个维度可以是任何长度（输入图片的个数）\n",
    "# 第二个维度784=28x28是每一个图片的展平后的向量\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "\n",
    "# 创建一个占位符来保存权重值W和偏置量b\n",
    "# Variable常用来保存参数，计算中可以被修改，初始值为0\n",
    "# 0~9共10个类别\n",
    "W = tf.Variable(tf.zeros([n_input, n_classes]))\n",
    "b = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "# y是预测出来的分类结果\n",
    "# x乘以W(相当于W_x)再加上b，然后输入到softmax里面\n",
    "y_evidence = tf.matmul(x, W) + b\n",
    "y_softmax = tf.nn.softmax(y_evidence)\n",
    "\n",
    "### 计算交叉熵----------------------------------\n",
    "\n",
    "# 创建一个占位符y_true用来保存输出的正确值（正确分类）\n",
    "y_true = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# 计算交叉熵，又叫cost\n",
    "# 直接按照公式写出的代码，如果出现log(0)的话结果就会变成NaN，不建议使用！！！！！\n",
    "# cross_entropy = tf.reduce_mean(\n",
    "#     -tf.reduce_sum(y_true * tf.log(y_softmax), reduction_indices=[1]))\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y_true, logits=y_evidence))\n",
    "\n",
    "# 学习步伐，又叫optimizer\n",
    "# 优化方法使用梯度下降算法（gradient descent algorithm）来最小化交叉熵\n",
    "# 会自动使用反向传播法（backpropagation algorithm）\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "    cross_entropy)\n",
    "\n",
    "### 训练和评估模型----------------------------------\n",
    "\n",
    "# 建立一个交互式的会话\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# 评估\n",
    "pred = tf.argmax(y_evidence, 1)\n",
    "true = tf.argmax(y_true, 1)\n",
    "correct_prediction = tf.equal(pred, true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 循环训练10次\n",
    "# 每次训练会随机抓取训练数据中100个数据点\n",
    "start = time.time()\n",
    "for epoch_i in range(training_epochs):\n",
    "    ave_cost = 0\n",
    "    for batch_i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run(\n",
    "            [train_step, cross_entropy],\n",
    "            feed_dict={\n",
    "                x: batch_xs,\n",
    "                y_true: batch_ys\n",
    "            })\n",
    "        ave_cost += c / total_batch\n",
    "        # evidence and softmax data\n",
    "        if (epoch_i == 0 and batch_i == 0) or (epoch_i == 9 and batch_i == 0):\n",
    "            train_evidence = y_evidence.eval(feed_dict={\n",
    "                x: batch_xs,\n",
    "                y_true: batch_ys\n",
    "            })\n",
    "            train_softmax = y_softmax.eval(feed_dict={\n",
    "                x: batch_xs,\n",
    "                y_true: batch_ys\n",
    "            })\n",
    "            print(\"train_evidence:\", train_evidence[:5])\n",
    "            print(\"train_softmax:\", train_softmax[:5])\n",
    "            train_pred = pred.eval(feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "            train_true = true.eval(feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "            test_pred = pred.eval(feed_dict={\n",
    "                x: mnist.test.images,\n",
    "                y_true: mnist.test.labels\n",
    "            })\n",
    "            test_true = true.eval(feed_dict={\n",
    "                x: mnist.test.images,\n",
    "                y_true: mnist.test.labels\n",
    "            })\n",
    "            print(\"train_pred:\", train_pred[:5])\n",
    "            print(\"train_true:\", train_true[:5])\n",
    "            print(\"test_pred:\", test_pred[:5])\n",
    "            print(\"test_true:\", test_true[:5])\n",
    "    if epoch_i % 1 == 0:\n",
    "        train_acc = accuracy.eval(feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        test_acc = accuracy.eval(feed_dict={\n",
    "            x: mnist.test.images,\n",
    "            y_true: mnist.test.labels\n",
    "        })\n",
    "        print(\"Epoch:%3d Batch:%5d \" % (epoch_i,\n",
    "                                        batch_i), \"train_acc=%.5f\" % train_acc,\n",
    "              \"test_acc=%.5f\" % test_acc, \"train_cost=%.9f\" % ave_cost)\n",
    "end = time.time()\n",
    "print(\"Process Time(s):{:.2f}\".format(end - start))\n",
    "\n",
    "### 输出结果----------------------------------\n",
    "print(\n",
    "    sess.run(\n",
    "        accuracy, feed_dict={\n",
    "            x: mnist.test.images,\n",
    "            y_true: mnist.test.labels\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5331\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "<class 'list'>\n",
      "5331\n",
      "simplistic , silly and tedious . \n",
      "\n",
      "<class 'list'>\n",
      "10662\n",
      "the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal \n",
      "\n",
      "<class 'list'>\n",
      "5331\n",
      "[0, 1] \n",
      "\n",
      "<class 'list'>\n",
      "5331\n",
      "[1, 0] \n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "(10662, 2)\n",
      "[0 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "positive_data_file = \"./data/rt-polarity.pos\"\n",
    "negative_data_file = \"./data/rt-polarity.neg\"\n",
    "\n",
    "positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "positive_examples = [s.strip() for s in positive_examples]\n",
    "negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "negative_examples = [s.strip() for s in negative_examples]\n",
    "\n",
    "print(type(positive_examples))\n",
    "print(len(positive_examples))\n",
    "print(positive_examples[0],\"\\n\")\n",
    "\n",
    "print(type(negative_examples))\n",
    "print(len(negative_examples))\n",
    "print(negative_examples[0],\"\\n\")\n",
    "\n",
    "x_text = positive_examples + negative_examples\n",
    "x_text = [clean_str(sent) for sent in x_text]\n",
    "\n",
    "print(type(x_text))\n",
    "print(len(x_text))\n",
    "print(x_text[0],\"\\n\")\n",
    "\n",
    "positive_labels = [[0, 1] for _ in positive_examples]\n",
    "negative_labels = [[1, 0] for _ in negative_examples]\n",
    "\n",
    "print(type(positive_labels))\n",
    "print(len(positive_labels))\n",
    "print(positive_labels[0],\"\\n\")\n",
    "\n",
    "print(type(negative_labels))\n",
    "print(len(negative_labels))\n",
    "print(negative_labels[0],\"\\n\")\n",
    "\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "print(type(y))\n",
    "print(y.shape)\n",
    "print(y[0],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 \n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-ff7ad1a6f1a3>:3: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/kinten/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "18758 \n",
      "\n",
      "(10662, 56)\n",
      "[ 1  2  3  4  5  6  1  7  8  9 10 11 12 13 14  9 15  5 16 17 18 19 20 21\n",
      " 22 23 24 25 26 27 28 29 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0] \n",
      "\n",
      "-1066 \n",
      "\n",
      "(9596, 56)\n",
      "[4719   59  182   34  190  804    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0] \n",
      "\n",
      "(1066, 56)\n",
      "[  292    84   523  1889    99   100   274    67    13 15402   121  4596\n",
      "   600   722  1456  2279   944   207  8493   503   125 10507     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0] \n",
      "\n",
      "(9596, 2)\n",
      "[1 0] \n",
      "\n",
      "(1066, 2)\n",
      "[1 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "print(max_document_length,\"\\n\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(len(vocab_processor.vocabulary_),\"\\n\")\n",
    "print(x.shape)\n",
    "print(x[0],\"\\n\")\n",
    "\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "dev_sample_percentage = 0.1\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "print(dev_sample_index,\"\\n\")\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0],\"\\n\")\n",
    "print(x_dev.shape)\n",
    "print(x_dev[0],\"\\n\")\n",
    "print(y_train.shape)\n",
    "print(y_train[0],\"\\n\")\n",
    "print(y_dev.shape)\n",
    "print(y_dev[0],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kinten/WorkSpaces/GitHub/python/text_cnn/text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-52cbab139527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#     for batch in batches:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "session_conf = tf.ConfigProto(\n",
    "    allow_soft_placement=True, log_device_placement=False)\n",
    "\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "embedding_dim = 128\n",
    "filter_sizes = \"3,4,5\"\n",
    "num_filters = 128\n",
    "l2_reg_lambda = 0.0\n",
    "dropout_keep_prob = 0.5\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "evaluate_every = 100\n",
    "checkpoint_every = 100\n",
    "\n",
    "with sess.as_default():\n",
    "    cnn = TextCNN(\n",
    "        sequence_length=x_train.shape[1],\n",
    "        num_classes=y_train.shape[1],\n",
    "        vocab_size=len(vocab_processor.vocabulary_),\n",
    "        embedding_size=embedding_dim,\n",
    "        filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "        num_filters=num_filters,\n",
    "        l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        grads_and_vars, global_step=global_step)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train_step(x_batch, y_batch):\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x_batch,\n",
    "            cnn.input_y: y_batch,\n",
    "            cnn.dropout_keep_prob: dropout_keep_prob\n",
    "        }\n",
    "        _, step, loss, accuracy = sess.run(\n",
    "            [train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "        print(\"step:{}\\t loss:{:g}\\t acc:{:g}\".format(step, loss,\n",
    "                                                  accuracy))\n",
    "\n",
    "    def dev_step(x_batch, y_batch):\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x_batch,\n",
    "            cnn.input_y: y_batch,\n",
    "            cnn.dropout_keep_prob: 1.0\n",
    "        }\n",
    "        step, loss, accuracy = sess.run([global_step, cnn.loss, cnn.accuracy],\n",
    "                                        feed_dict)\n",
    "        print(\"step:{}\\t loss:{:g}\\t acc:{:g}\".format(step, loss, accuracy))\n",
    "        \n",
    "    batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "    \n",
    "    print(batches.shape)\n",
    "    \n",
    "#     for batch in batches:\n",
    "#         x_batch, y_batch = zip(*batch)\n",
    "#         train_step(x_batch, y_batch)\n",
    "#         current_step = tf.train.global_step(sess, global_step)\n",
    "#         if current_step % evaluate_every == 0:\n",
    "#             print(\"\\nEvaluation:\")\n",
    "#             dev_step(x_dev, y_dev)\n",
    "#             print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Convolutional Neural Network　畳込みニューラルネットワーク\n",
    "\n",
    "### (1) 入力層\n",
    "28x28=784次元のベクトルが入力ベクトルです。<br>\n",
    "数字の画像を784ピクセルに分割し、1次元=1ピクセルで情報を代入しています。\n",
    "\n",
    "### (2) 畳込み層1 -> プーリング層1\n",
    "2層準備。ゼロパディング(zero padding)という補完機能を利用しています。<br>\n",
    "ファイルの畳込みに対して、周縁データが欠落するのですが、その補完のためにパディングを用います。<br>\n",
    "（パディングの話で推測できるように、畳込み層では、入力の次元圧縮を行いません）<br>\n",
    "プーリング(pooling)では、畳込みで返された数値を圧縮します。<br>\n",
    "ここでは、最大プーリング(max pooling)という、最も基本的な手法を用いています。<br>\n",
    "プーリングでも、パディングを行うことがあります。<br>\n",
    "（重ね合わせがずれる領域が存在することがあるため）\n",
    "\n",
    "* ゼロパディング(zero padding)とは、画像や帳票などで数値を表現する際、<br>\n",
    "書式で指定した桁数に満たない部分をゼロで埋めることである。<br>\n",
    "例えば「123」を5桁で表す場合、2桁足りない部分をゼロで埋めて「00123」と表記する<br>\n",
    "* 最大プーリング(max pooling)とは、設定範囲のピクセルの中で最大値を選択する\n",
    "\n",
    "### (3) 畳込み層2 -> プーリング層2\n",
    "(2)の繰り返しでさらに圧縮 -> この段階で7x7=49次元まで落とす予定です。\n",
    "\n",
    "### (4) 全結合層(高密度結合層)\n",
    "抽出したプーリング層からの出力層を入力層に送り、NNのような処理をかけます。<br>\n",
    "活性化関数はReLUです。\n",
    "\n",
    "### (5) クラス分類処理\n",
    "ソフトマックス関数の計算 -> 交差エントロピー誤差関数による評価\n",
    "\n",
    "https://qiita.com/icoxfog417/items/5fd55fad152231d706c2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "import numpy as np\n",
    "import time\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "# Network Parameters\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "train_dropout = 0.8\n",
    "test_dropout = 1.0\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "print(total_batch)\n",
    "training_epochs = 30\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def CNN(x, weights, biases, dropout):\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "y_pred = CNN(X, weights, biases, keep_prob)\n",
    "y_softmax = tf.nn.softmax(y_pred)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# type 1:\n",
    "# loss = tf.reduce_mean(\n",
    "#     -tf.reduce_sum(Y * tf.log(y_softmax), reduction_indices=[1]))\n",
    "# type 2:\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_pred))\n",
    "# type 3:\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=y_pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "pred = tf.argmax(y_pred, 1)\n",
    "true = tf.argmax(Y, 1)\n",
    "correct_prediction = tf.equal(pred, true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 550 train_acc=0.900 test_acc=0.950 train_cost=5309.987\n",
      "Epoch: 2 Batch: 550 train_acc=0.980 test_acc=0.968 train_cost=716.799\n",
      "Epoch: 3 Batch: 550 train_acc=1.000 test_acc=0.971 train_cost=361.856\n",
      "Epoch: 4 Batch: 550 train_acc=0.940 test_acc=0.976 train_cost=234.321\n",
      "Epoch: 5 Batch: 550 train_acc=0.970 test_acc=0.975 train_cost=151.020\n",
      "Epoch: 6 Batch: 550 train_acc=0.980 test_acc=0.980 train_cost=110.864\n",
      "Epoch: 7 Batch: 550 train_acc=0.980 test_acc=0.977 train_cost=82.911\n",
      "Epoch: 8 Batch: 550 train_acc=0.990 test_acc=0.981 train_cost=65.452\n",
      "Epoch: 9 Batch: 550 train_acc=1.000 test_acc=0.982 train_cost=53.947\n",
      "Epoch:10 Batch: 550 train_acc=1.000 test_acc=0.981 train_cost=42.150\n",
      "Epoch:11 Batch: 550 train_acc=0.980 test_acc=0.984 train_cost=32.161\n",
      "Epoch:12 Batch: 550 train_acc=0.980 test_acc=0.983 train_cost=31.916\n",
      "Epoch:13 Batch: 550 train_acc=1.000 test_acc=0.985 train_cost=25.750\n",
      "Epoch:14 Batch: 550 train_acc=0.990 test_acc=0.984 train_cost=19.147\n",
      "Epoch:15 Batch: 550 train_acc=1.000 test_acc=0.984 train_cost=17.817\n",
      "Epoch:16 Batch: 550 train_acc=0.990 test_acc=0.986 train_cost=17.203\n",
      "Epoch:17 Batch: 550 train_acc=0.990 test_acc=0.985 train_cost=17.812\n",
      "Epoch:18 Batch: 550 train_acc=1.000 test_acc=0.985 train_cost=15.475\n",
      "Epoch:19 Batch: 550 train_acc=1.000 test_acc=0.985 train_cost=13.051\n",
      "Epoch:20 Batch: 550 train_acc=0.980 test_acc=0.987 train_cost=11.155\n",
      "Epoch:21 Batch: 550 train_acc=1.000 test_acc=0.985 train_cost=11.102\n",
      "Epoch:22 Batch: 550 train_acc=1.000 test_acc=0.987 train_cost=9.455\n",
      "Epoch:23 Batch: 550 train_acc=0.990 test_acc=0.987 train_cost=10.040\n",
      "Epoch:24 Batch: 550 train_acc=1.000 test_acc=0.987 train_cost=9.001\n",
      "Epoch:25 Batch: 550 train_acc=1.000 test_acc=0.987 train_cost=7.657\n",
      "Epoch:26 Batch: 550 train_acc=0.990 test_acc=0.987 train_cost=6.531\n",
      "Epoch:27 Batch: 550 train_acc=1.000 test_acc=0.984 train_cost=7.621\n",
      "Epoch:28 Batch: 550 train_acc=0.990 test_acc=0.987 train_cost=7.900\n",
      "Epoch:29 Batch: 550 train_acc=1.000 test_acc=0.987 train_cost=6.181\n",
      "Epoch:30 Batch: 550 train_acc=1.000 test_acc=0.986 train_cost=6.505\n",
      "Process Time :84.69 s\n",
      "test accuracy=0.986\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Training cycle\n",
    "all_x_test = mnist.test.images\n",
    "all_y_test = mnist.test.labels\n",
    "start = time.time()\n",
    "for epoch_i in range(training_epochs):\n",
    "    ave_cost = 0\n",
    "    for batch_i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run(\n",
    "            [optimizer, loss],\n",
    "            feed_dict={\n",
    "                X: batch_x,\n",
    "                Y: batch_y,\n",
    "                keep_prob: train_dropout\n",
    "            })\n",
    "        ave_cost += c / total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch_i % 1 == 0:\n",
    "        train_acc = sess.run(accuracy,feed_dict={\n",
    "            X: batch_x,\n",
    "            Y: batch_y,\n",
    "            keep_prob: train_dropout\n",
    "        })\n",
    "        test_acc = sess.run(accuracy,feed_dict={\n",
    "            X: all_x_test,\n",
    "            Y: all_y_test,\n",
    "            keep_prob: test_dropout\n",
    "        })\n",
    "        print(\"Epoch:%2d Batch:%4d\" % (epoch_i + 1, batch_i + 1),\n",
    "              \"train_acc=%.3f\" % train_acc, \"test_acc=%.3f\" % test_acc,\n",
    "              \"train_cost=%5.3f\" % ave_cost)\n",
    "end = time.time()\n",
    "print(\"Process Time :%.2f s\" % (end - start))\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = sess.run(\n",
    "    accuracy,\n",
    "    feed_dict={\n",
    "        X: all_x_test,\n",
    "        Y: all_y_test,\n",
    "        keep_prob: test_dropout\n",
    "    })\n",
    "print(\"test accuracy=%.3f\" % acc)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
